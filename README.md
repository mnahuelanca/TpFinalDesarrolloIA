# ğŸ§  CNN para ClasificaciÃ³n de ImÃ¡genes â€“ CIFAR-10

> **Proyecto Final â€“ Red Neuronal Convolucional en TensorFlow/Keras**

Este proyecto implementa una **Red Neuronal Convolucional (CNN)** diseÃ±ada para clasificar imÃ¡genes del dataset **CIFAR-10**. El flujo de trabajo abarca desde el preprocesamiento y la aumentaciÃ³n de datos hasta la construcciÃ³n, entrenamiento y evaluaciÃ³n del modelo.

---

## ğŸ“Œ CaracterÃ­sticas del Proyecto

* **Dataset:** Uso estÃ¡ndar de CIFAR-10 (60.000 imÃ¡genes, 10 clases).
* **Preprocesamiento:** NormalizaciÃ³n de imÃ¡genes.
* **Data Augmentation:** Implementado para reducir el sobreajuste (overfitting).
* **Arquitectura Robusta:** 3 bloques convolucionales con Batch Normalization, MaxPooling y Dropout.
* **OptimizaciÃ³n:** Entrenamiento con Adam y `categorical_crossentropy`.
* **Early Stopping:** DetenciÃ³n temprana para evitar el sobreentrenamiento.
* **EvaluaciÃ³n:** MÃ©tricas detalladas, curvas de aprendizaje y matriz de confusiÃ³n.

---

## ğŸ—‚ Dataset: CIFAR-10

El conjunto de datos consta de imÃ¡genes a color de **32x32 pÃ­xeles**.

* **Volumen:** 60.000 imÃ¡genes en total.
    * 50.000 para Entrenamiento + ValidaciÃ³n.
    * 10.000 para Test.
* **Clases (10):**
    `airplane`, `automobile`, `bird`, `cat`, `deer`, `dog`, `frog`, `horse`, `ship`, `truck`.

---

## ğŸ— Arquitectura del Modelo

El modelo sigue una estructura secuencial con capas de aumento de datos al inicio:

### ğŸ”¹ Preprocesamiento
* **Data Augmentation:** `RandomFlip`, `RandomRotation`, `RandomZoom`.

### ğŸ”¹ Bloque 1
* `Conv2D` (32 filtros)
* `BatchNorm`
* `Conv2D` (32 filtros)
* `MaxPooling`
* `Dropout` (0.2)

### ğŸ”¹ Bloque 2
* `Conv2D` (64 filtros)
* `BatchNorm`
* `Conv2D` (64 filtros)
* `MaxPooling`
* `Dropout` (0.3)

### ğŸ”¹ Bloque 3
* `Conv2D` (128 filtros)
* `BatchNorm`
* `MaxPooling`
* `Dropout` (0.4)

### ğŸ”¹ ClasificaciÃ³n (Top)
* `Flatten`
* `Dense` (128) + RegularizaciÃ³n L2 + `Dropout`
* `Dense` (10, activaciÃ³n `softmax`)

---

## âš™ï¸ Entrenamiento

El modelo se compila y entrena con la siguiente configuraciÃ³n:

```python
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

- **Optimizador:** Adam
- **FunciÃ³n de PÃ©rdida:** Categorical Crossentropy (multiclase)
- **Callbacks:**
  - **EarlyStopping:** Monitorea `val_loss` con una paciencia de 10 Ã©pocas y restaura los mejores pesos (`restore_best_weights=True`)

## ğŸ“Š Resultados

El rendimiento del modelo en el conjunto de prueba (Test Set) fue el siguiente:

| MÃ©trica | Valor |
|---------|-------|
| Accuracy Test | 74.21% |
| Loss Test | 0.9066 |

### ğŸ“ InterpretaciÃ³n:

El modelo acierta aproximadamente **3 de cada 4 imÃ¡genes** que nunca ha visto antes. Un loss de 0.90 se considera un valor intermedio y aceptable para modelos iniciales en CIFAR-10 (tÃ­picamente entre 0.7 y 1.2).

## ğŸ“ˆ GrÃ¡ficos Generados

El notebook incluye la generaciÃ³n de las siguientes visualizaciones para el anÃ¡lisis:

- **Curva de Accuracy:** Entrenamiento vs. ValidaciÃ³n
- **Curva de Loss:** Entrenamiento vs. ValidaciÃ³n
- **Matriz de ConfusiÃ³n:** Para identificar en quÃ© clases falla mÃ¡s el modelo

## ğŸ›  InstalaciÃ³n y Uso

### 1. Clonar el repositorio
```bash
git clone https://github.com/mnahuelanca/TpFinalDesarrolloIA
```

### 2. Instalar dependencias
```bash
pip install -r requirements.txt
```


### 3. Ejecutar el proyecto

Puedes ejecutar el notebook directamente:
```bash
python index.ipynb
```

O abrirlo mediante **Jupyter Notebook** o **Google Colab**.

## ğŸ“¦ Requisitos

- tensorflow
- numpy
- matplotlib
- scikit-learn
- seaborn
